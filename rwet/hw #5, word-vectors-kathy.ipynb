{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word vectors\n",
    "\n",
    "Code from Allison Parrish's [word vector tutorial](https://github.com/aparrish/rwet/blob/master/understanding-word-vectors.ipynb) with modifications by kathy wu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "color_data = json.loads(open(\"xkcd.json\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hex_to_int(s):\n",
    "    s = s.lstrip(\"#\")\n",
    "    return int(s[:2], 16), int(s[2:4], 16), int(s[4:6], 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "colors = dict()\n",
    "for item in color_data['colors']:\n",
    "    colors[item[\"color\"]] = hex_to_int(item[\"hex\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"`meanv` function takes a list of vectors and finds their mean or average:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.0, 2.0]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def meanv(coords):\n",
    "    # assumes every item in coords has same length as item 0\n",
    "    sumv = [0] * len(coords[0])\n",
    "    for item in coords:\n",
    "        for i in range(len(item)):\n",
    "            sumv[i] += item[i]\n",
    "    mean = [0] * len(sumv)\n",
    "    for i in range(len(sumv)):\n",
    "        mean[i] = float(sumv[i]) / len(coords)\n",
    "    return mean\n",
    "meanv([[0, 1], [2, 2], [4, 3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### this function finds the closest item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def closest(space, coord, n=10):\n",
    "    closest = []\n",
    "    for key in sorted(space.keys(),\n",
    "                      key=lambda x: distance(coord, space[x]))[:n]:\n",
    "        closest.append(key)\n",
    "    return closest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['blue',\n",
       " 'vibrant blue',\n",
       " 'electric blue',\n",
       " 'azul',\n",
       " 'blue blue',\n",
       " 'vivid blue',\n",
       " 'bright blue',\n",
       " 'cerulean blue',\n",
       " 'rich blue',\n",
       " 'true blue']"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest(colors, colors['blue'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Doing bad digital humanities)\n",
    "\n",
    "## averaging body colors of alien species in popular Sci Fi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "starwars = nlp(open(\"starwars.txt\").read())\n",
    "# use word.lower_ to normalize case\n",
    "drac_colors1 = [colors[word.lower_] for word in starwars if word.lower_ in colors]\n",
    "avg_color1 = meanv(drac_colors1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[110.29268292682927, 95.59756097560975, 92.1951219512195]\n",
      "----\n",
      "Colors of aliens in Star Wars\n",
      "['greyish brown', 'dark taupe', 'dirty purple', 'grey brown', 'slate grey', 'gunmetal', 'dark mauve', 'dull brown', 'brownish grey', 'brownish purple']\n"
     ]
    }
   ],
   "source": [
    "print(avg_color1)\n",
    "print(\"----\")\n",
    "\n",
    "print(\"Colors of aliens in Star Wars\");\n",
    "starwars_colors = closest(colors, avg_color1)\n",
    "print(starwars_colors);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "stargate = nlp(open(\"stargate.txt\").read())\n",
    "# use word.lower_ to normalize case\n",
    "drac_colors2 = [colors[word.lower_] for word in stargate if word.lower_ in colors]\n",
    "avg_color2 = meanv(drac_colors2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[151.23333333333332, 111.63333333333334, 72.7]\n",
      "----\n",
      "Colors of aliens in Star Gate\n",
      "['mocha', 'dirt', 'brownish', 'dull brown', 'earth', 'puce', 'coffee', 'cocoa', 'tan brown', 'dark taupe']\n"
     ]
    }
   ],
   "source": [
    "print(avg_color2)\n",
    "print(\"----\")\n",
    "\n",
    "print(\"Colors of aliens in Star Gate\");\n",
    "stargate_colors = closest(colors, avg_color2)\n",
    "print(stargate_colors);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "doctorwho = nlp(open(\"doctorwho.txt\").read())\n",
    "# use word.lower_ to normalize case\n",
    "drac_colors3 = [colors[word.lower_] for word in doctorwho if word.lower_ in colors]\n",
    "avg_color3 = meanv(drac_colors3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[146.6530612244898, 108.16326530612245, 86.12244897959184]\n",
      "----\n",
      "Colors of aliens in Doctor Who\n",
      "['brownish', 'mocha', 'dull brown', 'brownish grey', 'dirt', 'grey brown', 'dark taupe', 'greyish brown', 'puce', 'cocoa']\n"
     ]
    }
   ],
   "source": [
    "print(avg_color3)\n",
    "print(\"----\")\n",
    "\n",
    "print(\"Colors of aliens in Doctor Who\");\n",
    "doctorwho_colors = closest(colors, avg_color3)\n",
    "\n",
    "print(doctorwho_colors);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## adding colors to sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliens of doctor who\n",
      "----\n",
      "On another planet, where people are brownish.\n",
      "How different they seem, with their mocha complexion.\n",
      "And their dull brown eyes,\n",
      "and their brownish grey hair.\n",
      "Watch as they eat their dirt food\n",
      "Watch as they drive their grey brown cars\n"
     ]
    }
   ],
   "source": [
    "print(\"aliens of doctor who\")\n",
    "print(\"----\")\n",
    "\n",
    "print(\"On another planet, where people are \" + doctorwho_colors[0]+\".\")\n",
    "print(\"How different they seem, with their \" + doctorwho_colors[1] + \" complexion.\")\n",
    "print(\"And their \" + doctorwho_colors[2] + \" eyes,\")\n",
    "print(\"and their \" + doctorwho_colors[3] + \" hair.\")\n",
    "print(\"Watch as they eat their \" + doctorwho_colors[4] + \" food\")\n",
    "print(\"Watch as they drive their \" + doctorwho_colors[5] + \" cars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliens of starwars\n",
      "----\n",
      "On another planet, where people are greyish brown.\n",
      "How different they seem, with their dark taupe complexion.\n",
      "And their dirty purple eyes,\n",
      "and their grey brown hair.\n",
      "Watch as they eat their slate grey food\n",
      "Watch as they drive their gunmetal cars\n"
     ]
    }
   ],
   "source": [
    "print(\"aliens of starwars\")\n",
    "print(\"----\")\n",
    "\n",
    "print(\"On another planet, where people are \" + starwars_colors[0]+\".\")\n",
    "print(\"How different they seem, with their \" + starwars_colors[1] + \" complexion.\")\n",
    "print(\"And their \" + starwars_colors[2] + \" eyes,\")\n",
    "print(\"and their \" + starwars_colors[3] + \" hair.\")\n",
    "print(\"Watch as they eat their \" + starwars_colors[4] + \" food\")\n",
    "print(\"Watch as they drive their \" + starwars_colors[5] + \" cars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliens of stargate\n",
      "----\n",
      "On another planet, where people are mocha.\n",
      "How different they seem, with their dirt complexion.\n",
      "And their brownish eyes,\n",
      "and their dull brown hair.\n",
      "Watch as they eat their earth food\n",
      "Watch as they drive their puce cars\n"
     ]
    }
   ],
   "source": [
    "print(\"aliens of stargate\")\n",
    "print(\"----\")\n",
    "\n",
    "print(\"On another planet, where people are \" + stargate_colors[0]+\".\")\n",
    "print(\"How different they seem, with their \" + stargate_colors[1] + \" complexion.\")\n",
    "print(\"And their \" + stargate_colors[2] + \" eyes,\")\n",
    "print(\"and their \" + stargate_colors[3] + \" hair.\")\n",
    "print(\"Watch as they eat their \" + stargate_colors[4] + \" food\")\n",
    "print(\"Watch as they drive their \" + stargate_colors[5] + \" cars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word vectors + spacy + zuckerberg rambles\n",
    "\n",
    "Using spacy to analyze facebook hearing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The following cell loads the language model and parses the input text:\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "doc = nlp(open(\"facebook.txt\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# all of the words in the text file\n",
    "tokens = list(set([w.text for w in doc if w.is_alpha]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vec(s):\n",
    "    return nlp.vocab[s].vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from tutorial notes:\n",
    "\n",
    "### Cosine similarity and finding closest neighbors\n",
    "\n",
    "The cell below defines a function `cosine()`, which returns the [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) of two vectors. Cosine similarity is another way of determining how similar two vectors are, which is more suited to high-dimensional spaces. [See the Encyclopedia of Distances for more information and even more ways of determining vector similarity.](http://www.uco.es/users/ma1fegan/Comunes/asignaturas/vision/Encyclopedia-of-distances-2009.pdf)\n",
    "\n",
    "(You'll need to install `numpy` to get this to work. If you haven't already: `pip install numpy`. Use `sudo` if you need to and make sure you've upgraded to the most recent version of `pip` with `sudo pip install --upgrade pip`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "# cosine similarity\n",
    "def cosine(v1, v2):\n",
    "    if norm(v1) > 0 and norm(v2) > 0:\n",
    "        return dot(v1, v2) / (norm(v1) * norm(v2))\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell defines a function that iterates through a list of tokens and returns the token whose vector is most similar to a given vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spacy_closest(token_list, vec_to_check, n=10):\n",
    "    return sorted(token_list,\n",
    "                  key=lambda x: cosine(vec_to_check, vec(x)),\n",
    "                  reverse=True)[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LAUGHTER',\n",
       " 'happiness',\n",
       " 'displeasure',\n",
       " 'encouragement',\n",
       " 'conversation',\n",
       " 'feelings',\n",
       " 'pleasure',\n",
       " 'fear',\n",
       " 'sudden',\n",
       " 'voices']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#finding similar words\n",
    "spacy_closest(tokens, vec(\"laughter\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['intervention',\n",
       " 'strictly',\n",
       " 'legally',\n",
       " 'Excuse',\n",
       " 'What',\n",
       " 'includes',\n",
       " 'sit',\n",
       " 'harvested',\n",
       " 'past',\n",
       " 'As',\n",
       " 'HAWAII',\n",
       " 'revelations',\n",
       " 'concrete',\n",
       " 'pace',\n",
       " 'algorithm',\n",
       " 'gave',\n",
       " 'misusing',\n",
       " 'R',\n",
       " 'fire',\n",
       " 'revealing',\n",
       " 'LINDSEY',\n",
       " 'Delaware',\n",
       " 'malware',\n",
       " 'Kids',\n",
       " 'spell',\n",
       " 'cause',\n",
       " 'bowl',\n",
       " 'unprecedented',\n",
       " 'scheduling',\n",
       " 'reach',\n",
       " 'Transportation',\n",
       " 'voices',\n",
       " 'imagine',\n",
       " 'blindness',\n",
       " 'endless',\n",
       " 'absolutely',\n",
       " 'interviewed',\n",
       " 'recess',\n",
       " 'patchwork',\n",
       " 'regards',\n",
       " 'establishing',\n",
       " 'embrace',\n",
       " 'altogether',\n",
       " 'displeasure',\n",
       " 'Service',\n",
       " 'Picture',\n",
       " 'leave',\n",
       " 'affecting',\n",
       " 'profitability',\n",
       " 'COO',\n",
       " 'yours',\n",
       " 'Those',\n",
       " 'expectation',\n",
       " 'personalization',\n",
       " 'okay',\n",
       " 'approved',\n",
       " 'CRUZ',\n",
       " 'board',\n",
       " 'out',\n",
       " 'face',\n",
       " 'journalist',\n",
       " 'More',\n",
       " 'settings',\n",
       " 'purposes',\n",
       " 'previously',\n",
       " 'comparison',\n",
       " 'softball',\n",
       " 'Myanmar',\n",
       " 'defined',\n",
       " 'experiences',\n",
       " 'messaged',\n",
       " 'personal',\n",
       " 'interrupting',\n",
       " 'MO',\n",
       " 'lots',\n",
       " 'receiving',\n",
       " 'Final',\n",
       " 'officials',\n",
       " 'standards',\n",
       " 'employ',\n",
       " 'abuses',\n",
       " 'take',\n",
       " 'advertiser',\n",
       " 'template',\n",
       " 'colleagues',\n",
       " 'media',\n",
       " 'away',\n",
       " 'dark',\n",
       " 'August',\n",
       " 'leading',\n",
       " 'simply',\n",
       " 'open',\n",
       " 'messages',\n",
       " 'unified',\n",
       " 'anybody',\n",
       " 'conservative',\n",
       " 'proposition',\n",
       " 'sustain',\n",
       " 'individual',\n",
       " 'me',\n",
       " 'negotiable',\n",
       " 'news',\n",
       " 'rap',\n",
       " 'practices',\n",
       " 'obviously',\n",
       " 'chronology',\n",
       " 'hear',\n",
       " 'Keep',\n",
       " 'geopolitical',\n",
       " 'operations',\n",
       " 'billions',\n",
       " 'configure',\n",
       " 'collection',\n",
       " 'senior',\n",
       " 'Nevada',\n",
       " 'supply',\n",
       " 'collect',\n",
       " 'passively',\n",
       " 'far',\n",
       " 'suppressed',\n",
       " 'statute',\n",
       " 'becoming',\n",
       " 'minors',\n",
       " 'constitutes',\n",
       " 'Chair',\n",
       " 'cities',\n",
       " 'terrorist',\n",
       " 'environment',\n",
       " 'applying',\n",
       " 'entering',\n",
       " 'scores',\n",
       " 'underway',\n",
       " 'ethical',\n",
       " 'implementing',\n",
       " 'knowledge',\n",
       " 'developer',\n",
       " 'advertisements',\n",
       " 'reality',\n",
       " 'instead',\n",
       " 'reading',\n",
       " 'white',\n",
       " 'struggling',\n",
       " 'intense',\n",
       " 'begin',\n",
       " 'protect',\n",
       " 'create',\n",
       " 'incite',\n",
       " 'place',\n",
       " 'material',\n",
       " 'paragraph',\n",
       " 'stuff',\n",
       " 'Commerce',\n",
       " 'downloaded',\n",
       " 'serious',\n",
       " 'psychological',\n",
       " 'Amazon',\n",
       " 'correlated',\n",
       " 'framework',\n",
       " 'stakes',\n",
       " 'breaks',\n",
       " 'inside',\n",
       " 'reactive',\n",
       " 'lobbyists',\n",
       " 'four',\n",
       " 'decade',\n",
       " 'AIQ',\n",
       " 'along',\n",
       " 'saw',\n",
       " 'received',\n",
       " 'reporting',\n",
       " 'child',\n",
       " 'Moreover',\n",
       " 'senator',\n",
       " 'fast',\n",
       " 'presents',\n",
       " 'CAPITO',\n",
       " 'additional',\n",
       " 'fixing',\n",
       " 'argued',\n",
       " 'interesting',\n",
       " 'All',\n",
       " 'leaving',\n",
       " 'flip',\n",
       " 'expansive',\n",
       " 'Everybody',\n",
       " 'internally',\n",
       " 'told',\n",
       " 'critical',\n",
       " 'Committees',\n",
       " 'MARIA',\n",
       " 'at',\n",
       " 'teeth',\n",
       " 'doable',\n",
       " 'enduring',\n",
       " 'links',\n",
       " 'anticipate',\n",
       " 'languages',\n",
       " 'Sunday',\n",
       " 'rather',\n",
       " 'Okay',\n",
       " 'bottom',\n",
       " 'attribute',\n",
       " 'IOWA',\n",
       " 'your',\n",
       " 'voted',\n",
       " 'degree',\n",
       " 'birth',\n",
       " 'teens',\n",
       " 'financial',\n",
       " 'involves',\n",
       " 'equally',\n",
       " 'visit',\n",
       " 'unfairly',\n",
       " 'ramp',\n",
       " 'overlaps',\n",
       " 'formally',\n",
       " 'values',\n",
       " 'sign',\n",
       " 'Data',\n",
       " 'misuse',\n",
       " 'Trump',\n",
       " 'small',\n",
       " 'cross',\n",
       " 'Washington',\n",
       " 'improper',\n",
       " 'Would',\n",
       " 'bin',\n",
       " 'MINN',\n",
       " 'property',\n",
       " 'accessed',\n",
       " 'Have',\n",
       " 'applied',\n",
       " 'Most',\n",
       " 'attitudes',\n",
       " 'election',\n",
       " 'Whereas',\n",
       " 'HELLER',\n",
       " 'cynicism',\n",
       " 'lied',\n",
       " 'other',\n",
       " 'brought',\n",
       " 'refine',\n",
       " 'lawyers',\n",
       " 'located',\n",
       " 'picture',\n",
       " 'redo',\n",
       " 'rejoins',\n",
       " 'That',\n",
       " 'platforms',\n",
       " 'view',\n",
       " 'opinions',\n",
       " 'contradiction',\n",
       " 'grant',\n",
       " 'corporate',\n",
       " 'social',\n",
       " 'trying',\n",
       " 'fixed',\n",
       " 'constituents',\n",
       " 'intent',\n",
       " 'renamed',\n",
       " 'staff',\n",
       " 'global',\n",
       " 'requirement',\n",
       " 'vetting',\n",
       " 'give',\n",
       " 'wake',\n",
       " 'know',\n",
       " 'enhance',\n",
       " 'concerned',\n",
       " 'organizations',\n",
       " 'answered',\n",
       " 'fine',\n",
       " 'parties',\n",
       " 'CANTWELL',\n",
       " 'He',\n",
       " 'someone',\n",
       " 'ca',\n",
       " 'regulation',\n",
       " 'benefit',\n",
       " 'total',\n",
       " 'value',\n",
       " 'Another',\n",
       " 'Speak',\n",
       " 'reforms',\n",
       " 'associated',\n",
       " 'seen',\n",
       " 'body',\n",
       " 'Secretary',\n",
       " 'Hill',\n",
       " 'dedicate',\n",
       " 'nuances',\n",
       " 'revolve',\n",
       " 'where',\n",
       " 'ran',\n",
       " 'Denver',\n",
       " 'stories',\n",
       " 'alternative',\n",
       " 'Glenn',\n",
       " 'firing',\n",
       " 'crime',\n",
       " 'But',\n",
       " 'cooperate',\n",
       " 'hurry',\n",
       " 'exams',\n",
       " 'tech',\n",
       " 'cut',\n",
       " 'automatically',\n",
       " 'Around',\n",
       " 'final',\n",
       " 'higher',\n",
       " 'sessions',\n",
       " 'L',\n",
       " 'gambling',\n",
       " 'PATRICK',\n",
       " 'present',\n",
       " 'development',\n",
       " 'though',\n",
       " 'confidential',\n",
       " 'my',\n",
       " 'interfere',\n",
       " 'favoring',\n",
       " 'relatively',\n",
       " 'drives',\n",
       " 'teenagers',\n",
       " 'came',\n",
       " 'complete',\n",
       " 'do',\n",
       " 'printed',\n",
       " 'policed',\n",
       " 'was',\n",
       " 'subjective',\n",
       " 'national',\n",
       " 'TEX',\n",
       " 'promptly',\n",
       " 'negative',\n",
       " 'November',\n",
       " 'Customs',\n",
       " 'article',\n",
       " 'suffer',\n",
       " 'ability',\n",
       " 'director',\n",
       " 'consequences',\n",
       " 'shop',\n",
       " 'Policy',\n",
       " 'provider',\n",
       " 'amounted',\n",
       " 'Had',\n",
       " 'personnel',\n",
       " 'targeting',\n",
       " 'future',\n",
       " 'googling',\n",
       " 'damage',\n",
       " 'inherently',\n",
       " 'intentionally',\n",
       " 'decrease',\n",
       " 'completing',\n",
       " 'formal',\n",
       " 'consume',\n",
       " 'local',\n",
       " 'historically',\n",
       " 'numbers',\n",
       " 'safe',\n",
       " 'services',\n",
       " 'consent',\n",
       " 'third',\n",
       " 'gotten',\n",
       " 'At',\n",
       " 'conceded',\n",
       " 'skyrocketed',\n",
       " 'applications',\n",
       " 'necessarily',\n",
       " 'undermined',\n",
       " 'sold',\n",
       " 'nonresponsive',\n",
       " 'computers',\n",
       " 'typical',\n",
       " 'handle',\n",
       " 'offer',\n",
       " 'thank',\n",
       " 'lonely',\n",
       " 'base',\n",
       " 'want',\n",
       " 'idealistically',\n",
       " 'peace',\n",
       " 'suggested',\n",
       " 'Public',\n",
       " 'only',\n",
       " 'remember',\n",
       " 'unwitting',\n",
       " 'CORTEZ',\n",
       " 'near',\n",
       " 'light',\n",
       " 'better',\n",
       " 'investigate',\n",
       " 'confidence',\n",
       " 'flagged',\n",
       " 'Palantir',\n",
       " 'personally',\n",
       " 'technically',\n",
       " 'different',\n",
       " 'designed',\n",
       " 'modern',\n",
       " 'express',\n",
       " 'hey',\n",
       " 'strategy',\n",
       " 'registered',\n",
       " 'backups',\n",
       " 'retool',\n",
       " 'friends',\n",
       " 'discussion',\n",
       " 'starting',\n",
       " 'Me',\n",
       " 'Am',\n",
       " 'Chris',\n",
       " 'Russian',\n",
       " 'Chamber',\n",
       " 'isolated',\n",
       " 'massively',\n",
       " 'exploited',\n",
       " 'CEO',\n",
       " 'stay',\n",
       " 'affirmatively',\n",
       " 'discourse',\n",
       " 'originally',\n",
       " 'stronger',\n",
       " 'Dorm',\n",
       " 'trafficking',\n",
       " 'massive',\n",
       " 'academics',\n",
       " 'interference',\n",
       " 'Third',\n",
       " 'stifling',\n",
       " 'single',\n",
       " 'messaging',\n",
       " 'conflict',\n",
       " 'firm',\n",
       " 'Yes',\n",
       " 'entered',\n",
       " 'affirmative',\n",
       " 'informative',\n",
       " 'impacting',\n",
       " 'abusing',\n",
       " 'viewing',\n",
       " 'Amendment',\n",
       " 'relief',\n",
       " 'nowhere',\n",
       " 'shift',\n",
       " 'until',\n",
       " 'doubled',\n",
       " 'identifying',\n",
       " 'complexity',\n",
       " 'automated',\n",
       " 'point',\n",
       " 'Immigration',\n",
       " 'them',\n",
       " 'actions',\n",
       " 'competitors',\n",
       " 'thereby',\n",
       " 'agree',\n",
       " 'slur',\n",
       " 'follow',\n",
       " 'Actually',\n",
       " 'left',\n",
       " 'subscribers',\n",
       " 'lacks',\n",
       " 'polarizing',\n",
       " 'restrict',\n",
       " 'legislative',\n",
       " 'exploring',\n",
       " 'chair',\n",
       " 'sell',\n",
       " 'interest',\n",
       " 'prevent',\n",
       " 'together',\n",
       " 'result',\n",
       " 'afford',\n",
       " 'various',\n",
       " 'patterns',\n",
       " 'ago',\n",
       " 'primary',\n",
       " 'scandal',\n",
       " 'deploying',\n",
       " 'special',\n",
       " 'dangerous',\n",
       " 'unwilling',\n",
       " 'jobs',\n",
       " 'helps',\n",
       " 'reject',\n",
       " 'sure',\n",
       " 'regimes',\n",
       " 'try',\n",
       " 'listed',\n",
       " 'customize',\n",
       " 'hearings',\n",
       " 'Sherman',\n",
       " 'whatsoever',\n",
       " 'ubiquitous',\n",
       " 'half',\n",
       " 'student',\n",
       " 'founding',\n",
       " 'custody',\n",
       " 'HIRONO',\n",
       " 'episode',\n",
       " 'regretted',\n",
       " 'violates',\n",
       " 'struggled',\n",
       " 'Kogan',\n",
       " 'lay',\n",
       " 'night',\n",
       " 'dismiss',\n",
       " 'disturbs',\n",
       " 'generally',\n",
       " 'deletion',\n",
       " 'sales',\n",
       " 'speech',\n",
       " 'assuming',\n",
       " 'DATA',\n",
       " 'Charlie',\n",
       " 'tens',\n",
       " 'forth',\n",
       " 'back',\n",
       " 'informing',\n",
       " 'efficiently',\n",
       " 'am',\n",
       " 'helpful',\n",
       " 'committing',\n",
       " 'Stanford',\n",
       " 'rejects',\n",
       " 'administration',\n",
       " 'controlling',\n",
       " 'truth',\n",
       " 'names',\n",
       " 'disposal',\n",
       " 'Park',\n",
       " 'relationships',\n",
       " 'Yesterday',\n",
       " 'Palo',\n",
       " 'cultural',\n",
       " 'mistake',\n",
       " 'tell',\n",
       " 'groups',\n",
       " 'relied',\n",
       " 'default',\n",
       " 'exploiting',\n",
       " 'French',\n",
       " 'discriminated',\n",
       " 'linguistic',\n",
       " 'submits',\n",
       " 'publish',\n",
       " 'contours',\n",
       " 'options',\n",
       " 'broadly',\n",
       " 'Commissioner',\n",
       " 'simplifying',\n",
       " 'over',\n",
       " 'divisive',\n",
       " 'they',\n",
       " 'purchases',\n",
       " 'hijacked',\n",
       " 'repeatedly',\n",
       " 'counsel',\n",
       " 'store',\n",
       " 'RI',\n",
       " 'scrutiny',\n",
       " 'inauthentic',\n",
       " 'across',\n",
       " 'liberals',\n",
       " 'compared',\n",
       " 'police',\n",
       " 'deploy',\n",
       " 'days',\n",
       " 'some',\n",
       " 'utilize',\n",
       " 'amount',\n",
       " 'unless',\n",
       " 'yeah',\n",
       " 'Machinery',\n",
       " 'witnessed',\n",
       " 'methods',\n",
       " 'persons',\n",
       " 'Blunt',\n",
       " 'fear',\n",
       " 'paper',\n",
       " 'leaning',\n",
       " 'website',\n",
       " 'RECESS',\n",
       " 'months',\n",
       " 'hate',\n",
       " 'native',\n",
       " 'proposal',\n",
       " 'Chinese',\n",
       " 'always',\n",
       " 'catch',\n",
       " 'introducing',\n",
       " 'strategic',\n",
       " 'tired',\n",
       " 'regulated',\n",
       " 'request',\n",
       " 'passed',\n",
       " 'Magazine',\n",
       " 'happen',\n",
       " 'required',\n",
       " 'America',\n",
       " 'doors',\n",
       " 'tact',\n",
       " 'really',\n",
       " 'nudity',\n",
       " 'enjoy',\n",
       " 'equivalent',\n",
       " 'establish',\n",
       " 'felt',\n",
       " 'constructed',\n",
       " 'join',\n",
       " 'games',\n",
       " 'primarily',\n",
       " 'realistic',\n",
       " 'prior',\n",
       " 'participated',\n",
       " 'definition',\n",
       " 'obtain',\n",
       " 'Judiciary',\n",
       " 'anyone',\n",
       " 'consuming',\n",
       " 'ILL',\n",
       " 'D',\n",
       " 'keep',\n",
       " 'Long',\n",
       " 'sets',\n",
       " 'met',\n",
       " 'alternate',\n",
       " 'incredible',\n",
       " 'review',\n",
       " 'Cornyn',\n",
       " 'put',\n",
       " 'bed',\n",
       " 'positive',\n",
       " 'countries',\n",
       " 'worse',\n",
       " 'series',\n",
       " 'literacy',\n",
       " 'pages',\n",
       " 'Absolutely',\n",
       " 'card',\n",
       " 'fit',\n",
       " 'Committee',\n",
       " 'linguistically',\n",
       " 'midterms',\n",
       " 'German',\n",
       " 'Sounds',\n",
       " 'entirely',\n",
       " 'blocking',\n",
       " 'breeches',\n",
       " 'investigators',\n",
       " 'Lives',\n",
       " 'financing',\n",
       " 'verification',\n",
       " 'brings',\n",
       " 'became',\n",
       " 'deleting',\n",
       " 'debates',\n",
       " 'Nelson',\n",
       " 'successful',\n",
       " 'could',\n",
       " 'basis',\n",
       " 'For',\n",
       " 'ideas',\n",
       " 'protections',\n",
       " 'addiction',\n",
       " 'sees',\n",
       " 'please',\n",
       " 'assumed',\n",
       " 'happened',\n",
       " 'hide',\n",
       " 'disagree',\n",
       " 'Given',\n",
       " 'prioritized',\n",
       " 'statement',\n",
       " 'VISA',\n",
       " 'idea',\n",
       " 'rural',\n",
       " 'behemoth',\n",
       " 'Last',\n",
       " 'roughly',\n",
       " 'imminent',\n",
       " 'err',\n",
       " 'longest',\n",
       " 'angry',\n",
       " 'trading',\n",
       " 'dispel',\n",
       " 'caused',\n",
       " 'industry',\n",
       " 'decisions',\n",
       " 'outlined',\n",
       " 'fill',\n",
       " 'incentive',\n",
       " 'misjudgments',\n",
       " 'photos',\n",
       " 'sync',\n",
       " 'scope',\n",
       " 'quote',\n",
       " 'communities',\n",
       " 'prominently',\n",
       " 'going',\n",
       " 'what',\n",
       " 'unsuspecting',\n",
       " 'relevant',\n",
       " 'gets',\n",
       " 'BLUNT',\n",
       " 'organize',\n",
       " 'Wicker',\n",
       " 'thinking',\n",
       " 'stead',\n",
       " 'found',\n",
       " 'insights',\n",
       " 'neutral',\n",
       " 'ISPs',\n",
       " 'dissent',\n",
       " 'To',\n",
       " 'Pakistan',\n",
       " 'disagreed',\n",
       " 'developing',\n",
       " 'button',\n",
       " 'method',\n",
       " 'kept',\n",
       " 'backlash',\n",
       " 'wo',\n",
       " 'attacks',\n",
       " 'knew',\n",
       " 'GDPR',\n",
       " 'Democratic',\n",
       " 'Heller',\n",
       " 'talks',\n",
       " 'documented',\n",
       " 'otherwise',\n",
       " 'difference',\n",
       " 'statements',\n",
       " 'unable',\n",
       " 'print',\n",
       " 'consulting',\n",
       " 'reservation',\n",
       " 'Swahili',\n",
       " 'ski',\n",
       " 'no',\n",
       " 'Before',\n",
       " 'effectively',\n",
       " 'will',\n",
       " 'opaque',\n",
       " 'compliance',\n",
       " 'losing',\n",
       " 'sort',\n",
       " 'MIT',\n",
       " 'assume',\n",
       " 'Feinstein',\n",
       " 'Burmese',\n",
       " 'HASSAN',\n",
       " 'considered',\n",
       " 'itself',\n",
       " 'search',\n",
       " 'quo',\n",
       " 'eight',\n",
       " 'love',\n",
       " 'being',\n",
       " 'hypocritical',\n",
       " 'aisle',\n",
       " 'ask',\n",
       " 'discussing',\n",
       " 'determining',\n",
       " 'ugly',\n",
       " 'apologize',\n",
       " 'answer',\n",
       " 'consents',\n",
       " 'frankly',\n",
       " 'since',\n",
       " 'Although',\n",
       " 'calculate',\n",
       " 'robust',\n",
       " 'adequately',\n",
       " 'texts',\n",
       " 'fun',\n",
       " 'promised',\n",
       " 'six',\n",
       " 'Initiative',\n",
       " 'advised',\n",
       " 'analysis',\n",
       " 'addition',\n",
       " 'crowbar',\n",
       " 'forward',\n",
       " 'chairmen',\n",
       " 'tailor',\n",
       " 'Whether',\n",
       " 'recruited',\n",
       " 'regarding',\n",
       " 'Section',\n",
       " 'corporation',\n",
       " 'issue',\n",
       " 'protecting',\n",
       " 'business',\n",
       " 'conversing',\n",
       " 'courtesy',\n",
       " 'kicking',\n",
       " 'Capitol',\n",
       " 'perspective',\n",
       " 'IRA',\n",
       " 'executives',\n",
       " 'First',\n",
       " 'Explain',\n",
       " 'recognized',\n",
       " 'clearer',\n",
       " 'Show',\n",
       " 'deem',\n",
       " 'fiduciary',\n",
       " 'disturbing',\n",
       " 'much',\n",
       " 'population',\n",
       " 'who',\n",
       " 'doing',\n",
       " 'versus',\n",
       " 'leaves',\n",
       " 'comprehensive',\n",
       " 'pleasure',\n",
       " 'pull',\n",
       " 'another',\n",
       " 'sides',\n",
       " 'privacy',\n",
       " 'dramatically',\n",
       " 'treated',\n",
       " 'accommodate',\n",
       " 'story',\n",
       " 'acknowledged',\n",
       " 'firmly',\n",
       " 'engines',\n",
       " 'guidance',\n",
       " 'advertising',\n",
       " 'conducting',\n",
       " 'APIs',\n",
       " 'appropriate',\n",
       " 'just',\n",
       " 'News',\n",
       " 'has',\n",
       " 'hopefully',\n",
       " 'additionally',\n",
       " 'regulator',\n",
       " 'ignored',\n",
       " 'imitative',\n",
       " 'poor',\n",
       " 'well',\n",
       " 'ultimately',\n",
       " 'ZUCKERBERG',\n",
       " 'incredibly',\n",
       " 'If',\n",
       " 'African',\n",
       " 'ban',\n",
       " 'compromised',\n",
       " 'remain',\n",
       " 'needing',\n",
       " 'efforts',\n",
       " 'authorization',\n",
       " 'expand',\n",
       " 'enforcement',\n",
       " 'repeat',\n",
       " 'teams',\n",
       " 'session',\n",
       " 'broadest',\n",
       " 'go',\n",
       " 'FHA',\n",
       " 'whether',\n",
       " 'Like',\n",
       " 'deployed',\n",
       " 'stored',\n",
       " 'reduce',\n",
       " 'thanks',\n",
       " 'Equifax',\n",
       " 'revelation',\n",
       " 'founded',\n",
       " 'makes',\n",
       " 'by',\n",
       " 'greatest',\n",
       " 'JOHNSON',\n",
       " 'building',\n",
       " 'BOOKER',\n",
       " 'senators',\n",
       " 'controls',\n",
       " 'possible',\n",
       " 'list',\n",
       " 'Campaign',\n",
       " 'reduces',\n",
       " 'technique',\n",
       " 'questions',\n",
       " 'Edge',\n",
       " 'continue',\n",
       " 'epidemic',\n",
       " 'certain',\n",
       " 'encourage',\n",
       " 'round',\n",
       " 'overregulate',\n",
       " 'color',\n",
       " 'few',\n",
       " 'Any',\n",
       " 'Delawareans',\n",
       " 'offline',\n",
       " 'progress',\n",
       " 'exhaustive',\n",
       " 'considerable',\n",
       " 'connections',\n",
       " 'advertises',\n",
       " 'calendar',\n",
       " 'valid',\n",
       " 'broken',\n",
       " 'lower',\n",
       " 'verifying',\n",
       " 'tension',\n",
       " 'even',\n",
       " 'communicated',\n",
       " 'anytime',\n",
       " 'committed',\n",
       " 'examine',\n",
       " 'Instead',\n",
       " 'driven',\n",
       " 'Warner',\n",
       " 'regard',\n",
       " 'ad',\n",
       " 'Gardner',\n",
       " 'its',\n",
       " 'OFF',\n",
       " 'browser',\n",
       " 'human',\n",
       " 'tablets',\n",
       " 'minute',\n",
       " 'delivers',\n",
       " 'tactics',\n",
       " 'misinformation',\n",
       " 'knows',\n",
       " 'expanding',\n",
       " 'sense',\n",
       " 'status',\n",
       " 'misused',\n",
       " 'earthly',\n",
       " 'neighbors',\n",
       " 'Hungary',\n",
       " 'exact',\n",
       " 'Of',\n",
       " 'publicly',\n",
       " 'Panther',\n",
       " 'federal',\n",
       " 'TESTER',\n",
       " 'standing',\n",
       " 'fat',\n",
       " 'closed',\n",
       " 'systems',\n",
       " 'extract',\n",
       " 'gain',\n",
       " 'internal',\n",
       " 'suck',\n",
       " 'while',\n",
       " 'theft',\n",
       " 'sharing',\n",
       " 'Mark',\n",
       " 'stayed',\n",
       " 'Common',\n",
       " 'survey',\n",
       " 'Commercial',\n",
       " 'trend',\n",
       " 'competition',\n",
       " 'Harvard',\n",
       " 'reads',\n",
       " 'attention',\n",
       " 'TED',\n",
       " 'wondering',\n",
       " 'sex',\n",
       " 'Eunoia',\n",
       " 'solved',\n",
       " 'understand',\n",
       " 'WASH',\n",
       " 'tragedy',\n",
       " 'Whenever',\n",
       " 'Protection',\n",
       " 'McCain',\n",
       " 'phones',\n",
       " 'either',\n",
       " 'creating',\n",
       " 'graders',\n",
       " 'ProPublica',\n",
       " 'killing',\n",
       " 'respect',\n",
       " 'spineless',\n",
       " 'reviewers',\n",
       " 'college',\n",
       " 'extracted',\n",
       " 'inappropriate',\n",
       " 'disseminating',\n",
       " 'finalize',\n",
       " 'send',\n",
       " 'unsatisfying',\n",
       " 'pieces',\n",
       " 'hiring',\n",
       " 'blocked',\n",
       " 'breeding',\n",
       " 'greater',\n",
       " 'saying',\n",
       " 'stickers',\n",
       " ...]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentvec(s):\n",
    "    sent = nlp(s)\n",
    "    return meanv([w.vector for w in sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = list(doc.sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spacy_closest_sent(space, input_str, n=10):\n",
    "    input_vec = sentvec(input_str)\n",
    "    return sorted(space,\n",
    "                  key=lambda x: cosine(np.mean([w.vector for w in x], axis=0), input_vec),\n",
    "                  reverse=True)[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"that's not what i asked.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "an abridged reading of april 11, 2018\n",
      "---\n",
      " \n",
      "Is that it?\n",
      "\n",
      " \n",
      "So is this — is then a question of Facebook is about feeling safe, or are users actually safe?\n",
      " \n",
      "But is it ever really gone?\n",
      "\n",
      " \n",
      "Is that right?\n",
      "\n",
      " \n",
      "but now it isn't with them.\n",
      " \n",
      "How long is that?\n",
      "\n",
      " \n",
      "Isn't that correct?\n",
      "\n",
      " \n",
      "Isn't that correct?\n",
      "\n",
      " \n",
      "So that just isn't a feature that's even available anymore.\n",
      " \n",
      "So it doesn't.\n",
      " \n",
      "(LAUGHTER)\n",
      "...\n",
      " \n",
      "(LAUGHTER)\n",
      "...\n",
      " \n",
      "(LAUGHTER)\n",
      "\n",
      " \n",
      "(LAUGHTER)\n",
      "\n",
      " \n",
      "(LAUGHTER)\n",
      "\n",
      " \n",
      "(LAUGHTER)\n",
      "\n",
      " \n",
      "(LAUGHTER)\n",
      "\n",
      " \n",
      "(LAUGHTER)\n",
      "\n",
      " \n",
      "(LAUGHTER)\n",
      "\n",
      " \n",
      "(LAUGHTER)\n",
      "\n",
      " \n",
      "— that's not what I'm asking.\n",
      " \n",
      "That's what I want to see.\n",
      " \n",
      "I think that that's very important.\n",
      " \n",
      "But here's the concern that I have.\n",
      " \n",
      "That's all I need.\n",
      " \n",
      "I think that that's an important conversation to have.\n",
      " \n",
      "I figured that would be the answer.\n",
      " \n",
      "I think common sense would tell us that that's pretty difficult.\n",
      " \n",
      "It's not that I expect anything that I say here today — to necessarily change people's view.\n",
      "\n",
      " \n",
      "If I own that data, I know it's being breached.\n",
      " \n"
     ]
    }
   ],
   "source": [
    "print(\"an abridged reading of april 11, 2018\");\n",
    "print(\"---\")\n",
    "print(\" \")\n",
    "\n",
    "for sent3 in spacy_closest_sent(sentences, \"But is it safe?\"):\n",
    "    print(sent3.text)\n",
    "    print(\" \")\n",
    "    \n",
    "for sent1 in spacy_closest_sent(sentences, \"LAUGHTER\"):\n",
    "    print(sent1.text)\n",
    "    print(\" \")\n",
    "for sent2 in spacy_closest_sent(sentences, \"That's not what I asked.\"):\n",
    "    print(sent2.text)\n",
    "    print(\" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
